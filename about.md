---
layout: default
title: About
permalink: /about/
---

# About Me
I’ve always been motivated by one thing: **finding ways to simplify complexity**.  
Even as a child, I was fascinated by patterns — searching for one rule that could explain an entire system,
or the simplest path through a messy problem. That curiosity never left me, and it naturally led me to
**AI and machine learning** as powerful tools for turning chaos into clarity — and to
**data engineering** as the foundation that makes those tools reliable and scalable.

## My Journey
I began my career focused on **machine learning research**, developing models for time-series forecasting,
behavior prediction, and real-time system optimization. But as I worked deeper, I realized that the true
enabler of machine learning isn’t just the model itself — it’s the **data pipelines and infrastructure**
that feed it. That realization shifted my focus to bridging both sides: building pipelines that deliver
trustworthy data, and optimizing models that make that data actionable.

## Key Contributions
At Samsung Research America, I helped **build a centralized datalake** capable of ingesting over 10,000 daily
fronthaul records. This infrastructure reduced analysis latency by 32% and gave downstream teams faster,
more reliable access to features for ML training. It wasn’t just about efficiency — it gave the organization
a single source of truth for data-driven decision-making.

I also led the **rebuild of a 10,000+ line time-series pipeline** to support NVIDIA Aerial upgrades.
This work involved refactoring feature generation modules and ensuring retraining compatibility across
software versions. The result was not only technical stability, but also long-term maintainability
for a system that multiple teams relied on.

In another project, I designed and deployed **end-to-end streaming pipelines** for real-time fraud detection,
integrating Kafka, Spark, Redis, and FastAPI. By unifying batch and streaming workflows, the system could
process data in near real-time while still allowing for replay and retraining.

On the machine learning side, I applied **PyTorch pruning and quantization** techniques to models in production.
This reduced model size by 62% and cut inference latency by 25–33% on embedded GPU devices, enabling
low-latency applications without compromising performance.

I also developed and deployed **ResNet- and NAFNet-based models** for real-time wireless channel estimation,
which improved accuracy by 17–40% compared to traditional methods. This experience deepened my appreciation
for applying ML research directly to production challenges under strict latency constraints.

Beyond company projects, I’ve explored applied data science work — from **building a churn prediction model
(AUC 0.87)** to **modeling Airbnb pricing drivers (R² = 0.79)** and creating a **Job Automation Vulnerability
Index** ranking 300+ U.S. cities. These projects gave me opportunities to experiment with feature engineering,
SHAP-based interpretation, and visualization to communicate insights to broader audiences.

## Beyond Work
Outside of work, I enjoy playing **badminton**. For me, it’s more than just a sport — it sharpens both body
and mind, while reinforcing the value of **teamwork, strategy, and collaboration**. The lessons I’ve learned
on the court often mirror how I approach engineering challenges: working with others, anticipating moves,
and adapting quickly when conditions change.

## Let’s Connect
My mission is to combine the strengths of **AI, machine learning, and data engineering** to design systems
that scale while making complexity manageable.  
**Please feel free to reach out** if you’d like to discuss opportunities, share ideas, or explore collaborations.
